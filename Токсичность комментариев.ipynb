{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c328ee-c310-4c92-a33b-8335070820fe",
   "metadata": {},
   "source": [
    "# Токсичность комметариев"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75184e4-7722-4693-9e17-0c6bad9eadb7",
   "metadata": {},
   "source": [
    "### Описание проекта\n",
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества F1 не меньше 0.75.\n",
    "\n",
    "Столбец text в нём содержит текст комментария, а toxic — целевой признак.\n",
    "\n",
    "### Содержание\n",
    "\n",
    "1. [Изучение и обработка данных](#start)\n",
    "2. [Обучение моделей](#model_training)       \n",
    "3. [Лучшая модель](#the_best_of_the_best)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9896cb56-6336-4cb7-8db9-3998c70ef51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from IPython.display import display\n",
    "import spacy\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import transformers\n",
    "from tqdm import notebook\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32100cbe-5b20-45cc-bcea-f157299f3634",
   "metadata": {},
   "source": [
    "## 1. Изучение и обработка данных<a id=\"start\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b874812-d368-4605-b914-6b308abe13a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0\n",
       "5           5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6           6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7           7  Your vandalism to the Matt Shirvington article...      0\n",
       "8           8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9           9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    toxic_comments = pd.read_csv('D:/Users/Иван/Downloads/Токсичность комметариев/toxic_comments.csv')\n",
    "except:\n",
    "    toxic_comments = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "toxic_comments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77638abc-1af5-46c1-a1d9-cb5ab93ec9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "toxic_comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8511b89e-8baa-459f-bf67-6a1244197c72",
   "metadata": {},
   "source": [
    "В тексте наблюдаем верхний и нижний регистр шрифта, не текстовые символы и прочий мусор, от которого для корректного обучения необходимо избавиться. Также в целевом признаке на десяти текстах видим дисбаланс, что вызывает подозрения. С проверки дисбаланса и начнём:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6527fc2-2658-4aa5-993c-e34fdce1d9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    143106\n",
       "1     16186\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверим баланс\n",
    "toxic_comments['toxic'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7ca45-901b-4d2c-a890-3c2fc158a6ed",
   "metadata": {},
   "source": [
    "Видим, что дисбаланс на целом кортеже имеет соотношение 9:1, что не очень хорошо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bfe0841-7111-405a-83e4-2e6a2321aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установим seed для воспроизводимости\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c16a0-ef56-4895-9720-f2c7e3513d38",
   "metadata": {},
   "source": [
    "Данные сбалансировали, теперь приступим к правильному виду текста, для этого лемматизируем его, приведём к нижнему регистру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ac00855-7ae4-4729-a9a9-4071c7bf8004",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = text.lower()\n",
    "    lemm_text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])# лемматизируем по словам и объединяем в строку\n",
    "    cleared_text = re.sub(r'[^a-zA-Z]', ' ', lemm_text) \n",
    "    return \" \".join(cleared_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b010be87-64af-4f68-b52d-43c2cb123fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация BERT токенизатора\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Функция для предварительной обработки текста для BERT\n",
    "def preprocess_for_bert(texts, labels, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,                     \n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels.values)\n",
    "    \n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924943cb-8438-4648-b9ac-facbb865543e",
   "metadata": {},
   "source": [
    "Далее леммитизируем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6071d328-91ed-4a0d-8ff2-895c0c773042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>congratulation from me a well use the tool wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>sorry if the word nonsense wa offensive to you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  toxic                                         lemmatized\n",
       "0           0      0  explanation why the edits made under my userna...\n",
       "1           1      0  d aww he match this background colour i m seem...\n",
       "2           2      0  hey man i m really not trying to edit war it s...\n",
       "3           3      0  more i can t make any real suggestion on impro...\n",
       "4           4      0  you sir are my hero any chance you remember wh...\n",
       "5           5      0  congratulation from me a well use the tool wel...\n",
       "6           6      1       cocksucker before you piss around on my work\n",
       "7           7      0  your vandalism to the matt shirvington article...\n",
       "8           8      0  sorry if the word nonsense wa offensive to you...\n",
       "9           9      0  alignment on this subject and which are contra..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments['lemmatized'] = toxic_comments['text'].apply(lemmatize_text)\n",
    "toxic_comments = toxic_comments.drop(['text'], axis=1)#удалим дублирующую колонку\n",
    "toxic_comments.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fadd66-0388-4289-a698-a186b154f7bd",
   "metadata": {},
   "source": [
    "Далее выделим целевой признак для дальнейшего разбиения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c21ebc1f-31bd-4e0d-a8f8-7052b63d1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = toxic_comments['lemmatized']\n",
    "target = toxic_comments['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cfba63-9fd9-4eac-b386-0df477c7eb61",
   "metadata": {},
   "source": [
    "Разобьем выборку по отношению 60:20:20. В связи с недостаточностью памяти valid и test выборки сформируем из 5000 строк, так как после балансировки train число строк сильно уменьшится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cebaca6-a415-4d75-ac64-6bd4409fe39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95575,) (2500,) (2500,)\n",
      "(95575,) (2500,) (2500,)\n"
     ]
    }
   ],
   "source": [
    "#из выборки выбираем 60% под трейн, которые дальше семплируем из оставшейся выборки выделяем 5000 записей, \n",
    "# которые делим на valid и test\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, target, test_size=0.4, \n",
    "                                                                              random_state=12345)\n",
    "\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(features_valid, target_valid, test_size=5000, \n",
    "                                                                              random_state=12345 )\n",
    "\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(features_test, target_test, test_size=0.5, \n",
    "                                                                              random_state=12345 )\n",
    "print(features_train.shape, features_valid.shape, features_test.shape)\n",
    "print(target_train.shape, target_valid.shape, target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307c25b-c747-4e83-bcf1-364c0b3a350e",
   "metadata": {},
   "source": [
    "Разбив выборки на train/valid/test приступаем к даунсемплингу нулевого класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49826635-1f56-4e96-8491-28129c0d523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat([features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n",
    "    target_downsampled = pd.concat([target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n",
    "    features_downsampled, target_downsampled = shuffle(features_downsampled, target_downsampled, random_state=12345)\n",
    "    return features_downsampled, target_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b73795d-1d7d-4bea-823a-140291f355b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    17185\n",
       "1     9652\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_downsampled, target_downsampled = downsample(features_train, target_train, 0.2)\n",
    "target_downsampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03593a0c-77e5-485c-8d7e-b433c6b06a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных для BERT\n",
    "\n",
    "train_texts = toxic_comments.loc[features_downsampled.index, 'lemmatized']\n",
    "train_labels = target_downsampled\n",
    "\n",
    "valid_texts = toxic_comments.loc[features_valid.index, 'lemmatized']\n",
    "valid_labels = target_valid\n",
    "\n",
    "test_texts = toxic_comments.loc[features_test.index, 'lemmatized']\n",
    "test_labels = target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "659834eb-ed2e-4163-98d3-f66d8a3b19cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Преобразование данных для BERT\n",
    "train_inputs, train_masks, train_labels_bert = preprocess_for_bert(train_texts, train_labels)\n",
    "val_inputs, val_masks, val_labels_bert = preprocess_for_bert(valid_texts, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4194343-45cd-46a7-9703-a6376ac756e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание DataLoader для BERT\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels_bert)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(val_inputs, val_masks, val_labels_bert)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a718c3ae-9c05-42f3-b58e-ade79a8891da",
   "metadata": {},
   "source": [
    "Загрузим стоп-слова и применим модель TfidfVectorizer, обучив её на наших данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa9fb683-0ffd-4503-99c2-dca822e8417a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Загрузка стоп-слов (это нужно сделать один раз)\n",
    "nltk.download('stopwords')\n",
    "stopwords = list(nltk_stopwords.words('english'))  # Преобразуем в список\n",
    "\n",
    "# Инициализация TfidfVectorizer с заданными стоп-словами\n",
    "tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "# Обучение и преобразование обучающего набора\n",
    "features_train = tf_idf.fit_transform(features_downsampled).toarray()\n",
    "\n",
    "# Преобразование валидационного и тестового наборов\n",
    "features_valid = tf_idf.transform(features_valid).toarray()\n",
    "features_test = tf_idf.transform(features_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e383b-78e9-4540-b1ed-21ebce6c4b6b",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "\n",
    "1. Выявили и устранили дисбаланс классов;\n",
    "2. Подготовили текст к обучению;\n",
    "3. Разбили выборки на тренировочную, валидационную и тестовую;\n",
    "4. Применили модель TfidfVectorizer для обучения на данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c77ac-e194-4f02-a1f1-38282aa581d2",
   "metadata": {},
   "source": [
    "## 2. Обучение моделей<a id=\"model_training\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6354237-1c76-483b-a7b3-523ca8dab6b6",
   "metadata": {},
   "source": [
    "### Random Forest Classifier¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f85d3780-420b-4dc0-a9e2-56a6c8eac8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=8, random_state=12345)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=8, random_state=12345)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=8, random_state=12345)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rfc =  RandomForestClassifier(random_state=12345, n_estimators=100, max_depth=8)\n",
    "model_rfc.fit(features_train, target_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de3e73b6-90bd-4cf2-adb7-baa180790bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на валидационной выборке 0.007434944237918215\n"
     ]
    }
   ],
   "source": [
    "predicted_valid_rfc = model_rfc.predict(features_valid)\n",
    "f1_rfc_valid = f1_score(target_valid, predicted_valid_rfc)\n",
    "\n",
    "print('F1 на валидационной выборке', f1_rfc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab439b6-a99b-4a3a-9ae5-a15aa04b7f10",
   "metadata": {},
   "source": [
    "### CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7af6fec3-3d87-4656-a8b1-a5cbcab7d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for estimator in range(10, 101, 10):\n",
    "#    model =  CatBoostClassifier(random_state=12345, n_estimators=estimator)\n",
    "#    model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95e49a60-09f0-4251-a049-d12c022b6716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.34674\n",
      "0:\tlearn: 0.5775899\ttotal: 195ms\tremaining: 19.4s\n",
      "1:\tlearn: 0.5205743\ttotal: 396ms\tremaining: 19.4s\n",
      "2:\tlearn: 0.4842611\ttotal: 595ms\tremaining: 19.2s\n",
      "3:\tlearn: 0.4639008\ttotal: 794ms\tremaining: 19.1s\n",
      "4:\tlearn: 0.4465827\ttotal: 998ms\tremaining: 19s\n",
      "5:\tlearn: 0.4346294\ttotal: 1.2s\tremaining: 18.8s\n",
      "6:\tlearn: 0.4202655\ttotal: 1.4s\tremaining: 18.5s\n",
      "7:\tlearn: 0.4131415\ttotal: 1.6s\tremaining: 18.4s\n",
      "8:\tlearn: 0.4053250\ttotal: 1.8s\tremaining: 18.2s\n",
      "9:\tlearn: 0.3976346\ttotal: 2.01s\tremaining: 18.1s\n",
      "10:\tlearn: 0.3901463\ttotal: 2.2s\tremaining: 17.8s\n",
      "11:\tlearn: 0.3849021\ttotal: 2.41s\tremaining: 17.7s\n",
      "12:\tlearn: 0.3797039\ttotal: 2.62s\tremaining: 17.5s\n",
      "13:\tlearn: 0.3754023\ttotal: 2.82s\tremaining: 17.3s\n",
      "14:\tlearn: 0.3713793\ttotal: 3.02s\tremaining: 17.1s\n",
      "15:\tlearn: 0.3642000\ttotal: 3.22s\tremaining: 16.9s\n",
      "16:\tlearn: 0.3607108\ttotal: 3.42s\tremaining: 16.7s\n",
      "17:\tlearn: 0.3572985\ttotal: 3.62s\tremaining: 16.5s\n",
      "18:\tlearn: 0.3536664\ttotal: 3.82s\tremaining: 16.3s\n",
      "19:\tlearn: 0.3496550\ttotal: 4.02s\tremaining: 16.1s\n",
      "20:\tlearn: 0.3465331\ttotal: 4.22s\tremaining: 15.9s\n",
      "21:\tlearn: 0.3443025\ttotal: 4.42s\tremaining: 15.7s\n",
      "22:\tlearn: 0.3412394\ttotal: 4.63s\tremaining: 15.5s\n",
      "23:\tlearn: 0.3372726\ttotal: 4.83s\tremaining: 15.3s\n",
      "24:\tlearn: 0.3349992\ttotal: 5.04s\tremaining: 15.1s\n",
      "25:\tlearn: 0.3329620\ttotal: 5.23s\tremaining: 14.9s\n",
      "26:\tlearn: 0.3296993\ttotal: 5.43s\tremaining: 14.7s\n",
      "27:\tlearn: 0.3278115\ttotal: 5.63s\tremaining: 14.5s\n",
      "28:\tlearn: 0.3239721\ttotal: 5.83s\tremaining: 14.3s\n",
      "29:\tlearn: 0.3218351\ttotal: 6.04s\tremaining: 14.1s\n",
      "30:\tlearn: 0.3195157\ttotal: 6.24s\tremaining: 13.9s\n",
      "31:\tlearn: 0.3174423\ttotal: 6.43s\tremaining: 13.7s\n",
      "32:\tlearn: 0.3153034\ttotal: 6.64s\tremaining: 13.5s\n",
      "33:\tlearn: 0.3133040\ttotal: 6.83s\tremaining: 13.3s\n",
      "34:\tlearn: 0.3110924\ttotal: 7.04s\tremaining: 13.1s\n",
      "35:\tlearn: 0.3087959\ttotal: 7.24s\tremaining: 12.9s\n",
      "36:\tlearn: 0.3063671\ttotal: 7.44s\tremaining: 12.7s\n",
      "37:\tlearn: 0.3039787\ttotal: 7.65s\tremaining: 12.5s\n",
      "38:\tlearn: 0.3023147\ttotal: 7.85s\tremaining: 12.3s\n",
      "39:\tlearn: 0.3006411\ttotal: 8.05s\tremaining: 12.1s\n",
      "40:\tlearn: 0.2990257\ttotal: 8.25s\tremaining: 11.9s\n",
      "41:\tlearn: 0.2976920\ttotal: 8.44s\tremaining: 11.7s\n",
      "42:\tlearn: 0.2958831\ttotal: 8.64s\tremaining: 11.5s\n",
      "43:\tlearn: 0.2941658\ttotal: 8.85s\tremaining: 11.3s\n",
      "44:\tlearn: 0.2922490\ttotal: 9.04s\tremaining: 11s\n",
      "45:\tlearn: 0.2908177\ttotal: 9.24s\tremaining: 10.8s\n",
      "46:\tlearn: 0.2891257\ttotal: 9.44s\tremaining: 10.7s\n",
      "47:\tlearn: 0.2879845\ttotal: 9.64s\tremaining: 10.4s\n",
      "48:\tlearn: 0.2865921\ttotal: 9.84s\tremaining: 10.2s\n",
      "49:\tlearn: 0.2848668\ttotal: 10s\tremaining: 10s\n",
      "50:\tlearn: 0.2836006\ttotal: 10.2s\tremaining: 9.84s\n",
      "51:\tlearn: 0.2810045\ttotal: 10.4s\tremaining: 9.64s\n",
      "52:\tlearn: 0.2794589\ttotal: 10.6s\tremaining: 9.44s\n",
      "53:\tlearn: 0.2773248\ttotal: 10.8s\tremaining: 9.24s\n",
      "54:\tlearn: 0.2762894\ttotal: 11s\tremaining: 9.03s\n",
      "55:\tlearn: 0.2748625\ttotal: 11.2s\tremaining: 8.83s\n",
      "56:\tlearn: 0.2737015\ttotal: 11.4s\tremaining: 8.63s\n",
      "57:\tlearn: 0.2724735\ttotal: 11.6s\tremaining: 8.43s\n",
      "58:\tlearn: 0.2709484\ttotal: 11.8s\tremaining: 8.22s\n",
      "59:\tlearn: 0.2701594\ttotal: 12s\tremaining: 8.03s\n",
      "60:\tlearn: 0.2687447\ttotal: 12.2s\tremaining: 7.83s\n",
      "61:\tlearn: 0.2676539\ttotal: 12.4s\tremaining: 7.63s\n",
      "62:\tlearn: 0.2663967\ttotal: 12.6s\tremaining: 7.42s\n",
      "63:\tlearn: 0.2648634\ttotal: 12.8s\tremaining: 7.22s\n",
      "64:\tlearn: 0.2634170\ttotal: 13.1s\tremaining: 7.03s\n",
      "65:\tlearn: 0.2626531\ttotal: 13.2s\tremaining: 6.83s\n",
      "66:\tlearn: 0.2615031\ttotal: 13.4s\tremaining: 6.62s\n",
      "67:\tlearn: 0.2600122\ttotal: 13.6s\tremaining: 6.42s\n",
      "68:\tlearn: 0.2590395\ttotal: 13.8s\tremaining: 6.22s\n",
      "69:\tlearn: 0.2584181\ttotal: 14s\tremaining: 6.02s\n",
      "70:\tlearn: 0.2572167\ttotal: 14.2s\tremaining: 5.82s\n",
      "71:\tlearn: 0.2562763\ttotal: 14.4s\tremaining: 5.62s\n",
      "72:\tlearn: 0.2556241\ttotal: 14.7s\tremaining: 5.42s\n",
      "73:\tlearn: 0.2543386\ttotal: 14.9s\tremaining: 5.22s\n",
      "74:\tlearn: 0.2537761\ttotal: 15.1s\tremaining: 5.02s\n",
      "75:\tlearn: 0.2528753\ttotal: 15.3s\tremaining: 4.82s\n",
      "76:\tlearn: 0.2522200\ttotal: 15.5s\tremaining: 4.62s\n",
      "77:\tlearn: 0.2513996\ttotal: 15.7s\tremaining: 4.42s\n",
      "78:\tlearn: 0.2506133\ttotal: 15.9s\tremaining: 4.22s\n",
      "79:\tlearn: 0.2499117\ttotal: 16.1s\tremaining: 4.02s\n",
      "80:\tlearn: 0.2494089\ttotal: 16.3s\tremaining: 3.82s\n",
      "81:\tlearn: 0.2485747\ttotal: 16.5s\tremaining: 3.62s\n",
      "82:\tlearn: 0.2480799\ttotal: 16.7s\tremaining: 3.42s\n",
      "83:\tlearn: 0.2472294\ttotal: 16.9s\tremaining: 3.22s\n",
      "84:\tlearn: 0.2466914\ttotal: 17.1s\tremaining: 3.02s\n",
      "85:\tlearn: 0.2456552\ttotal: 17.3s\tremaining: 2.82s\n",
      "86:\tlearn: 0.2443650\ttotal: 17.5s\tremaining: 2.62s\n",
      "87:\tlearn: 0.2433207\ttotal: 17.7s\tremaining: 2.42s\n",
      "88:\tlearn: 0.2421056\ttotal: 17.9s\tremaining: 2.21s\n",
      "89:\tlearn: 0.2416823\ttotal: 18.1s\tremaining: 2.01s\n",
      "90:\tlearn: 0.2413055\ttotal: 18.3s\tremaining: 1.81s\n",
      "91:\tlearn: 0.2400575\ttotal: 18.5s\tremaining: 1.61s\n",
      "92:\tlearn: 0.2392930\ttotal: 18.7s\tremaining: 1.41s\n",
      "93:\tlearn: 0.2382930\ttotal: 18.9s\tremaining: 1.21s\n",
      "94:\tlearn: 0.2378200\ttotal: 19.1s\tremaining: 1.01s\n",
      "95:\tlearn: 0.2371251\ttotal: 19.3s\tremaining: 806ms\n",
      "96:\tlearn: 0.2367232\ttotal: 19.5s\tremaining: 604ms\n",
      "97:\tlearn: 0.2359815\ttotal: 19.7s\tremaining: 403ms\n",
      "98:\tlearn: 0.2355498\ttotal: 19.9s\tremaining: 201ms\n",
      "99:\tlearn: 0.2346879\ttotal: 20.1s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x2180cb24850>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cbr =  CatBoostClassifier(random_state=12345, n_estimators=100)\n",
    "model_cbr.fit(features_train, target_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11b81b5b-d12f-4d49-a420-85a1ed5d6a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на валидационной выборке 0.7587476979742173\n"
     ]
    }
   ],
   "source": [
    "predicted_valid_cbr = model_cbr.predict(features_valid)\n",
    "f1_cbr_valid = f1_score(target_valid, predicted_valid_cbr)\n",
    "\n",
    "print('F1 на валидационной выборке', f1_cbr_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb244ec-6c03-4b21-b425-4ee605138e17",
   "metadata": {},
   "source": [
    "### BERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9366faa-e1cb-4dd7-83ad-a9f6b7f1b54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Загрузка модели BERT\n",
    "model_bert = BertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "044ef5cc-bacb-4a3c-8565-8a1f3330405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Настройка оптимизатора\n",
    "optimizer = AdamW(model_bert.parameters(),\n",
    "                lr=2e-5,\n",
    "                eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a2b433d-4337-411d-85e4-ed290f8dc6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка планировщика обучения\n",
    "epochs = 3\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                          num_warmup_steps=0,\n",
    "                                          num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e43bbff6-d35b-43f5-ad12-f07bba33c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для вычисления точности\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0cdcac97-fe7b-4706-b8e6-5366011c0e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Batch 40 of 839. Elapsed: 00:00:07\n",
      "  Batch 80 of 839. Elapsed: 00:00:14\n",
      "  Batch 120 of 839. Elapsed: 00:00:21\n",
      "  Batch 160 of 839. Elapsed: 00:00:28\n",
      "  Batch 200 of 839. Elapsed: 00:00:35\n",
      "  Batch 240 of 839. Elapsed: 00:00:42\n",
      "  Batch 280 of 839. Elapsed: 00:00:49\n",
      "  Batch 320 of 839. Elapsed: 00:00:56\n",
      "  Batch 360 of 839. Elapsed: 00:01:03\n",
      "  Batch 400 of 839. Elapsed: 00:01:10\n",
      "  Batch 440 of 839. Elapsed: 00:01:17\n",
      "  Batch 480 of 839. Elapsed: 00:01:24\n",
      "  Batch 520 of 839. Elapsed: 00:01:31\n",
      "  Batch 560 of 839. Elapsed: 00:01:38\n",
      "  Batch 600 of 839. Elapsed: 00:01:45\n",
      "  Batch 640 of 839. Elapsed: 00:01:52\n",
      "  Batch 680 of 839. Elapsed: 00:01:59\n",
      "  Batch 720 of 839. Elapsed: 00:02:06\n",
      "  Batch 760 of 839. Elapsed: 00:02:13\n",
      "  Batch 800 of 839. Elapsed: 00:02:20\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 00:02:27\n",
      "======== Epoch 2 / 3 ========\n",
      "  Batch 40 of 839. Elapsed: 00:00:07\n",
      "  Batch 80 of 839. Elapsed: 00:00:14\n",
      "  Batch 120 of 839. Elapsed: 00:00:21\n",
      "  Batch 160 of 839. Elapsed: 00:00:28\n",
      "  Batch 200 of 839. Elapsed: 00:00:35\n",
      "  Batch 240 of 839. Elapsed: 00:00:42\n",
      "  Batch 280 of 839. Elapsed: 00:00:49\n",
      "  Batch 320 of 839. Elapsed: 00:00:56\n",
      "  Batch 360 of 839. Elapsed: 00:01:03\n",
      "  Batch 400 of 839. Elapsed: 00:01:10\n",
      "  Batch 440 of 839. Elapsed: 00:01:17\n",
      "  Batch 480 of 839. Elapsed: 00:01:24\n",
      "  Batch 520 of 839. Elapsed: 00:01:31\n",
      "  Batch 560 of 839. Elapsed: 00:01:38\n",
      "  Batch 600 of 839. Elapsed: 00:01:45\n",
      "  Batch 640 of 839. Elapsed: 00:01:52\n",
      "  Batch 680 of 839. Elapsed: 00:01:59\n",
      "  Batch 720 of 839. Elapsed: 00:02:06\n",
      "  Batch 760 of 839. Elapsed: 00:02:14\n",
      "  Batch 800 of 839. Elapsed: 00:02:21\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 00:02:27\n",
      "======== Epoch 3 / 3 ========\n",
      "  Batch 40 of 839. Elapsed: 00:00:07\n",
      "  Batch 80 of 839. Elapsed: 00:00:14\n",
      "  Batch 120 of 839. Elapsed: 00:00:21\n",
      "  Batch 160 of 839. Elapsed: 00:00:28\n",
      "  Batch 200 of 839. Elapsed: 00:00:35\n",
      "  Batch 240 of 839. Elapsed: 00:00:42\n",
      "  Batch 280 of 839. Elapsed: 00:00:49\n",
      "  Batch 320 of 839. Elapsed: 00:00:56\n",
      "  Batch 360 of 839. Elapsed: 00:01:03\n",
      "  Batch 400 of 839. Elapsed: 00:01:10\n",
      "  Batch 440 of 839. Elapsed: 00:01:17\n",
      "  Batch 480 of 839. Elapsed: 00:01:24\n",
      "  Batch 520 of 839. Elapsed: 00:01:31\n",
      "  Batch 560 of 839. Elapsed: 00:01:38\n",
      "  Batch 600 of 839. Elapsed: 00:01:45\n",
      "  Batch 640 of 839. Elapsed: 00:01:53\n",
      "  Batch 680 of 839. Elapsed: 00:02:00\n",
      "  Batch 720 of 839. Elapsed: 00:02:07\n",
      "  Batch 760 of 839. Elapsed: 00:02:14\n",
      "  Batch 800 of 839. Elapsed: 00:02:21\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 00:02:28\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели BERT\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_bert.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model_bert.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - t0))\n",
    "            print(f'  Batch {step} of {len(train_dataloader)}. Elapsed: {elapsed}')\n",
    "            \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        model_bert.zero_grad()        \n",
    "        outputs = model_bert(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask, \n",
    "                            labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_bert.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    training_time = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - t0))\n",
    "    \n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "    print(f\"  Training epoch took: {training_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "32a3053a-503f-462f-9b10-5b610f37419b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 BERT на тестовой выборке: 0.7453\n"
     ]
    }
   ],
   "source": [
    "# Оценка BERT на тестовых данных\n",
    "test_inputs, test_masks, test_labels_bert = preprocess_for_bert(test_texts, test_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels_bert)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "model_bert.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask)\n",
    "    \n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "# Вычисление F1-score для BERT\n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "f1_bert = f1_score(flat_true_labels, flat_predictions)\n",
    "print(f'F1 BERT на тестовой выборке: {f1_bert:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf104b-acce-440a-ba16-36666f2e7e67",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce643360-ba20-43d8-b0e3-d2019508cb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=12345, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=12345, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=12345, solver='liblinear')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr = LogisticRegression(solver='liblinear', random_state=12345)\n",
    "model_lr.fit(features_train, target_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "546cba20-b583-4e4e-8029-0a93e385a374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на валидационной выборке 0.7554744525547445\n"
     ]
    }
   ],
   "source": [
    "predicted_valid_lr = model_lr.predict(features_valid)\n",
    "f1_lr_valid = f1_score(target_valid, predicted_valid_lr)\n",
    "\n",
    "print('F1 на валидационной выборке', f1_lr_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1dae03-e799-4a33-9ee3-10e106f5679e",
   "metadata": {},
   "source": [
    "Как видно из таблицы, лучше всех себя показал CatBoostRegressor с результатом F1 0.75. Соответственно до тестовой выборки допускается именно Cat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d69ffe-f165-4b59-ab53-1ebe67f4db25",
   "metadata": {},
   "source": [
    "## 3. Лучшая модель<a id=\"the_best_of_the_best\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0cc4835c-f5e5-4292-b565-f589d1a021ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на валидационной выборке 0.7317073170731708\n"
     ]
    }
   ],
   "source": [
    "predicted_test_cbr = model_cbr.predict(features_test)\n",
    "\n",
    "f1_cbr_test = f1_score(target_test, predicted_test_cbr)\n",
    "\n",
    "print('F1 на валидационной выборке', f1_cbr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c6739-6bab-457f-9173-cb20f6061299",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "- Обучить модель классифицировать комментарии на позитивные и негативные нам удалось;\n",
    "- Найти и построить модель со значением метрики качества F1 не меньше 0.75 удалось;\n",
    "- Лучшую модель, **CatBoostRegressor**, протестировали на тестовой выборке, получили результат метрики качества **F1 0.73**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b663fd-af6e-464e-9e2f-51a44d0f9aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
